{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1fa57d1-09a6-493e-b049-5cbdc85ccfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 10:45:20,798,INFO,Started processing: processed_titanic_data.csv\n",
      "2025-05-04 10:45:20,798,INFO,Started processing: processed_titanic_data.csv\n",
      "2025-05-04 10:45:20,798,INFO,Started processing: processed_titanic_data.csv\n",
      "2025-05-04 10:45:20,807,INFO,CSV loaded successfully\n",
      "2025-05-04 10:45:20,807,INFO,CSV loaded successfully\n",
      "2025-05-04 10:45:20,807,INFO,CSV loaded successfully\n",
      "2025-05-04 10:45:20,813,INFO,Categorical Columns: ['Name']\n",
      "2025-05-04 10:45:20,813,INFO,Categorical Columns: ['Name']\n",
      "2025-05-04 10:45:20,813,INFO,Categorical Columns: ['Name']\n",
      "2025-05-04 10:45:20,817,INFO,Numeric Columns: ['Sex', 'Age', 'Fare', 'Embarked']\n",
      "2025-05-04 10:45:20,817,INFO,Numeric Columns: ['Sex', 'Age', 'Fare', 'Embarked']\n",
      "2025-05-04 10:45:20,817,INFO,Numeric Columns: ['Sex', 'Age', 'Fare', 'Embarked']\n",
      "2025-05-04 10:45:20,819,INFO,Filled missing numeric: Sex\n",
      "2025-05-04 10:45:20,819,INFO,Filled missing numeric: Sex\n",
      "2025-05-04 10:45:20,819,INFO,Filled missing numeric: Sex\n",
      "2025-05-04 10:45:20,824,INFO,Filled missing numeric: Age\n",
      "2025-05-04 10:45:20,824,INFO,Filled missing numeric: Age\n",
      "2025-05-04 10:45:20,824,INFO,Filled missing numeric: Age\n",
      "2025-05-04 10:45:20,829,INFO,Filled missing numeric: Fare\n",
      "2025-05-04 10:45:20,829,INFO,Filled missing numeric: Fare\n",
      "2025-05-04 10:45:20,829,INFO,Filled missing numeric: Fare\n",
      "2025-05-04 10:45:20,833,INFO,Filled missing numeric: Embarked\n",
      "2025-05-04 10:45:20,833,INFO,Filled missing numeric: Embarked\n",
      "2025-05-04 10:45:20,833,INFO,Filled missing numeric: Embarked\n",
      "2025-05-04 10:45:20,836,INFO,Filled missing categorical: Name\n",
      "2025-05-04 10:45:20,836,INFO,Filled missing categorical: Name\n",
      "2025-05-04 10:45:20,836,INFO,Filled missing categorical: Name\n",
      "2025-05-04 10:45:20,842,INFO,Encoded: Name\n",
      "2025-05-04 10:45:20,842,INFO,Encoded: Name\n",
      "2025-05-04 10:45:20,842,INFO,Encoded: Name\n",
      "2025-05-04 10:45:20,853,INFO,Scaling done\n",
      "2025-05-04 10:45:20,853,INFO,Scaling done\n",
      "2025-05-04 10:45:20,853,INFO,Scaling done\n",
      "2025-05-04 10:45:20,863,INFO,Processed data saved to 'processed_output.csv'\n",
      "2025-05-04 10:45:20,863,INFO,Processed data saved to 'processed_output.csv'\n",
      "2025-05-04 10:45:20,863,INFO,Processed data saved to 'processed_output.csv'\n",
      "2025-05-04 10:45:20,891,INFO,Column summary saved to 'column_summary.csv'\n",
      "2025-05-04 10:45:20,891,INFO,Column summary saved to 'column_summary.csv'\n",
      "2025-05-04 10:45:20,891,INFO,Column summary saved to 'column_summary.csv'\n",
      "2025-05-04 10:45:20,896,INFO,ETL pipeline completed successfully!\n",
      "2025-05-04 10:45:20,896,INFO,ETL pipeline completed successfully!\n",
      "2025-05-04 10:45:20,896,INFO,ETL pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Setup logging with UTF-8 encoding for both file and stream handlers\n",
    "log_file = 'etl_log.csv'\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s,%(levelname)s,%(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file, mode='w', encoding='utf-8'),\n",
    "        logging.StreamHandler(sys.stdout)  # Default encoding, may still cause issues\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Try adding specific encoding to StreamHandler\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setFormatter(logging.Formatter('%(asctime)s,%(levelname)s,%(message)s'))\n",
    "console_handler.stream = sys.stdout  # Reset stream with 'utf-8' compatibility\n",
    "logging.getLogger().addHandler(console_handler)\n",
    "\n",
    "def process_csv(input_file):\n",
    "    try:\n",
    "        logging.info(f\"Started processing: {input_file}\")\n",
    "\n",
    "        # Load data\n",
    "        df = pd.read_csv(input_file)\n",
    "        logging.info(\"CSV loaded successfully\")\n",
    "\n",
    "        # Detect categorical and numeric columns\n",
    "        cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "        num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        logging.info(f\"Categorical Columns: {cat_cols}\")\n",
    "        logging.info(f\"Numeric Columns: {num_cols}\")\n",
    "\n",
    "        # Fill missing values\n",
    "        for col in num_cols:\n",
    "            df[col].fillna(df[col].mean(), inplace=True)\n",
    "            logging.info(f\"Filled missing numeric: {col}\")\n",
    "        for col in cat_cols:\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "            logging.info(f\"Filled missing categorical: {col}\")\n",
    "\n",
    "        # Encode categorical columns\n",
    "        for col in cat_cols:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col])\n",
    "            with open(f'label_encoder_{col}.pkl', 'wb') as f:\n",
    "                pickle.dump(le, f)\n",
    "            logging.info(f\"Encoded: {col}\")\n",
    "\n",
    "        # Scale numeric columns\n",
    "        scaler = StandardScaler()\n",
    "        df[num_cols + cat_cols] = scaler.fit_transform(df[num_cols + cat_cols])\n",
    "        with open('scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "        logging.info(\"Scaling done\")\n",
    "\n",
    "        # Save processed output\n",
    "        df.to_csv('processed_output.csv', index=False)\n",
    "        logging.info(\"Processed data saved to 'processed_output.csv'\")\n",
    "\n",
    "        # Save column summary\n",
    "        summary = df.describe().transpose()\n",
    "        summary.to_csv('column_summary.csv')\n",
    "        logging.info(\"Column summary saved to 'column_summary.csv'\")\n",
    "\n",
    "        logging.info(\"ETL pipeline completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_path = \"processed_titanic_data.csv\"  # Change this path if needed\n",
    "\n",
    "    if os.path.exists(input_path):\n",
    "        process_csv(input_path)\n",
    "    else:\n",
    "        logging.error(f\"File not found: {input_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
